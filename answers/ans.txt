Software versions used
	python : 2.7.6 
	pandas : 0.13.1
	scipy : 0.13.1
	numpy : 0.13.1

1. How will you analyze this data for cleanliness? 
	Mainly data cleanliness could be checked via three methods

	1> Data not available: Nan values
		In the time series given it might be possible that values are not at all recorded at particular timestamp
		for single or group of serieses. Values are not recorded, though they were present, so these values can
		not be replaced by 0 or any default value.

	2> Data has impossible/out of range values
		The given time series is represents incrimental difference between two readings taken at the timestamp.
		The difference could be +ve or -ve value, but there is certain range between which we will have values. 
		If the value recorded , difference between two successive reading, is lying out of range then it could
		be wrong reading. Spikes and deeps, when plotted graph, in the data represent such impossible values
	
	3> Data is in inconsitent format.
		Here, time series values are difference and are in floating format, strictly numeric format. If the values recorded 
		have any alphanumeric value, because of recording error, then it might be possible that we are looking at totally wrong observation.

a. Apply your suggestions to this data and describe ways in which it is not clean.
	
	I have written script  scripts/prob1.a/dataQaulityCheck.py. It has mean, std deviation, nan_count .
	Where I have counted perct of bad records depending upon how many nan values are there in series, how many points are beyond
	+ve and -ve range of  std_dev*5. The points std_dev*5 could be considered to check for the correctness. We can always specify
	such limit, depending upon the mean or standard deviation so that such suspesious values could be caught.

	a.1 Data not available: 
		There are few time serieses where lot of records are given NaN. if you see the graph for the time series.
		I have purposefully plotted NaN values at y=stddev*5. If you see a line at y=stddev*5, then those points
		are representing the NaN. A continuos NaN line gives you idea that serveral values are missing.
	
	a.2 Data has impossible:
		I did not find the evidences of the impossible values in the series. But I have found that there are
		some values which fall behind the limit of stddev*5. Even such values are the rare, but they could be
		checked for correctness. Here I have assumed that values beyond stddev*5 are the suspesious one and in
		real life these limits could be different.
	
	a.3 Data is in inconsitent format.
		Did not get any evidence of the incosistent format because of any recording error.

2. Describe which parts (if any) of the data you would want to find ways to clean and which parts (if any) you would abandon as being beyond cleanable and why.

	The only part which could be tested for recovery is NaN values, which represent the missing records.

	1> Parts which could be recovered.
		These are the scattered NaN values. There could be many methods/models to recover or reconstruct
		the missing values
		a. Analyising the similar time series. 
			If two series have are following each other, or have some
			kind of correlation between them, then it is possible to reconstruct the missing part of the 
			the time series by analyzing other series. It might happen that both series showing similar
			changes with respect to time or both of them are moving completly opposite to each other.
			In both the cases it is possible to reconstruct the missing points.
		b. Implementing interpolation methogsL:
			You could use the interpolation methods, either linear or taylor series approximations to
			recostruct the few missing values based of the values around the missing points. Data frame
			of values used to reconstruct will give you the current trend/direction, which could be used
			in general to recover missnig points.

	2> Parts which could not be recoverd.
		These are the continuos NaN samples in the time series. It is not possible to find the similar
		time series which follows the same pattern as large chuck reprenting the movement for continously
		large period of the time is abscent. Even to reconstruct the missing values, which are lot greater
		in the number, we have only few point as in input to any recovery model/method. As well we can not
		be sure if results are correct after forcefull application of such model.
		
		If you take example of time series 14, 17, 22, 25 they have respectively 61%, 45%, 64%, 75%  of the NaN
		values.
	
3. Write a program to provide your best estimate for the mean and standard-deviation of each of the time-series. 
		While calculating mean and stddev, I have simply ignored the NaN records.

		a. What if you had only (very) limited RAM to run your program? Can you run your program without loading the whole time series in RAM?
				Yes it is possible to calculate the mean and standard deviation without loading while time series. You can divide the series
				in the equal parts and calculate the mean and standard deviation part by part. Later you could sum up the results for each part to get final
				mean and stddev for whole of the series.

				To sum up the results of the parts, you might have to refactor the stddev and mean formulas and later integrate them to calculate 
				the standard deviation and mean.

				Since I am ignoring the NaN values, count of samples from each part will differ.

				Mean = (SumOfPart1 + SumOfPart2 + SumOfPart3 + ... + SumOfPart_N) / (sampleFromPart1 + sampleFromPart2 + sampleFromPart3 + ... + sampleFromPart_N)

				While formula for standard deviation, to calculate it by part by part is bit tricky.
	
				lets say, series has N samples and each denoted by Xi where i could range from 1 to N.
				while X is the mean of series.

				standard_deviation = ((1/ (N-1))  (X - Xi)^2)^(0.5).
				
				the formula could be restructred to find the (X-Xi)^2, Where mean ,X, will be unknown till we finish with the mean and i will range from 1 to  N.
				Xi i.e. X1, X2, X3 ... XN are the time series samples.
				
				so we can disintegate the (a-b)^2 to below.
					(X-Xi)^2 =  	X^2 + X1^2 - 2X1*X
								+	X^2 + X2^2 - 2X2*X
								+	X^2 + X3^2 - 2X3*X
								+	X^2 + X4^2 - 2X4*X
								.
								.
								.
								+	X^2 + XN^2 - 2XN*X

								= (X1^2 + X^2 + X3^2 + .... + XN^2) + (N*X^2) - (2*X*(X1+ X2 + X3 + ... + XN))
					In above formula, Xi represents the each sample in time series, which we already have.
					While X is the mean of the whole series.

					(X1+ X2 + X3 + ... + XN) = (SumOfPart1 + SumOfPart2 + SumOfPart3 + ... + SumOfPart_N) in the above mean formula.
					while (X1^2 + X^2 + X3^2 + .... + XN^2) is something we will calculate while iterating through series part by part.
					
					When mean , X , is calculated at the end of iteration through series, we can easily have (X-Xi)^2 values, thus standard
					deviation, without traversing the whole series twice.



4. Write a program to find the relation, if any, between series 2 and series 3. Describe your reasoning for what evidence you have found for this relation.

		I tried to find the correlation between series 2 and 3. You have given 5 million sample points during whole day and sample points seems to be
		difference between two successive recordings. Since number of samples are too granular, represent sample taken apart few milliseconds seconds,
		it is difficult and time consuming to calculate the correlation between two time serieses. As well it might give wrong correlation result to 
		analyse the differences taken at such minute level. so I aggregated the number of samples to represent the movement over certain larger period of time
		, lets say 10 mins. Aggregating, cumulative summing, all milliseconds samples within that particular 10 mins, will give you collective movement over 10 mins.
		Now it becomes easy to find the correlation between two series when we have clear direction change between two points, 
		which is represented by movement during larger time period of 10 mins.

		The correlation finding algorithm used is spearmans rank algorithm as we wanted to know only direction of change with respect to each other between
		two series. It might happen than series 3 has same direction change as of series 2, but at high magnitude.

		There is a relation between two series 2 and 3. For first half series 2 and 3 are following each other, moving in same direction and have +ve correlation. 
		While for rest half they are showing -ve correlation. ie. they move in opposite. To do this, I have program which basically devides the time series into
		multiple parts ie larger time frames. And tries to find the correlation between two serieses for
			
		a. What if you had only (very) limited RAM to run your program? Can you run your program without loading the whole time series in RAM? 
			Above each part of the program could be loaded saperatly, one by one, for the analysis. So if you divide your whole day in 6 parts,
			or per hour parts. Then you need to bring only hourly or 1/6 th data into the RAM. I have chosen single part as unit of data to be loaded
			into the RAM, but same could be done on the more granular level of rows.

		b. What if you had multiple CPUs available - could you speed up your program by parallelizing it?
			Each part represting particular hour above could be done processed  parallely

StackOverflow

Python :
	http://stackoverflow.com/questions/4383571/importing-files-from-different-folder-in-python
	http://stackoverflow.com/questions/20309456/how-to-call-a-function-from-another-file-in-python
	http://stackoverflow.com/questions/5574702/how-to-print-to-stderr-in-python
	https://docs.python.org/2/library/getopt.html

Python-multithreading :
	http://stackoverflow.com/questions/11968689/python-multithreading-wait-till-all-threads-finished

Pandas:
	http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.io.parsers.read_csv.html
	http://stackoverflow.com/questions/26266362/how-to-count-the-nan-values-in-the-column-in-panda-data-frame

stats:
	https://en.wikipedia.org/wiki/Standard_deviation
	http://smallbusiness.chron.com/difference-between-sample-population-standard-deviation-22639.html
	https://statistics.laerd.com/statistical-guides/measures-of-spread-standard-deviation.php



Questions
You are given a dataset (attached/linked, dataLarge.gz, 727MB compressed) that consists of 25 time series.

Set A:
1. How will you analyze this data for cleanliness? 
a. Apply your suggestions to this data and describe ways in which it is not clean.
2. Describe which parts (if any) of the data you would want to find ways to clean and which parts (if any) you would abandon as being beyond cleanable and why.
3. Write a program to provide your best estimate for the mean and standard-deviation of each of the time-series. 
a. What if you had only (very) limited RAM to run your program? Can you run your program without loading the whole time series in RAM?
b. What if you had multiple CPUs available - could you speed up your program by parallelizing it?
4. Write a program to find the relation, if any, between series 2 and series 3. Describe your reasoning for what evidence you have found for this relation.
a. What if you had only (very) limited RAM to run your program? Can you run your program without loading the whole time series in RAM? 
b. What if you had multiple CPUs available - could you speed up your program by parallelizing it?
5. Assume this dataset was data collected over a day and this collection was to be repeated every day. Describe ways in which you will store this data over time. 


Set B:
6. Write a program to compute the running PNL given a series of trades (quantity bought/sold) and a series of prices (attached/linked, trades.txt, 18.7KB uncompressed)
